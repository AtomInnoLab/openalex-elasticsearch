input {
  jdbc {
    jdbc_driver_library => "/usr/share/jars/postgresql-42.3.5.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    jdbc_connection_string => "${JDBC_URL_PROD}"
    jdbc_user => "${JDBC_USER_PROD}"
    jdbc_password => "${JDBC_PASSWORD_PROD}"
    jdbc_paging_enabled => false
    last_run_metadata_path => "/usr/share/logstash/sql_last_value.yml"
    schedule => "*/10 * * * * *"
    # The following query fetches records in batches of 100,000.
    # It uses '>=' to prevent data loss for records with identical timestamps.
    # NOTE: If more than 100,000 records have the exact same 'updated' timestamp,
    # this could lead to an infinite loop of processing the same first 100,000 records.
    # The idempotency of Elasticsearch (using document_id) prevents data duplication,
    # but it would be inefficient. This is a limitation of not being able to use a composite tracking key (updated, id).
    statement => "SELECT updated, json_save::text AS json_save FROM mid.json_works WHERE (updated AT TIME ZONE 'UTC') >= :sql_last_value::timestamptz ORDER BY updated, id LIMIT 100000"
    use_column_value => true
    tracking_column => updated
    tracking_column_type => "timestamp"
  }
}

filter {
  # 解析 JSON
  json {
    source => "json_save"
    remove_field => ["json_save"]
  }

  # publication_year 转整数并根据年份划分索引后缀
  if [publication_year] {
    mutate { convert => { "publication_year" => "integer" } }
    ruby {
      code => "
        year = event.get('publication_year')
        suffix = 'invalid-data'
        year = year[0] if year.is_a?(Array)
        if year.is_a?(Integer)
          suffix =
            case year
            when 0..1959 then '1959-or-less'
            when 1960..1969 then '1960s'
            when 1970..1979 then '1970s'
            when 1980..1989 then '1980s'
            when 1990..1994 then '1990-to-1994'
            when 1995..1999 then '1995-to-1999'
            when 2000..2025 then year.to_s
            else 'invalid-data'
            end
        end
        event.set('[@metadata][index_suffix]', suffix)
      "
    }
  } else {
    mutate { add_field => { "[@metadata][index_suffix]" => "invalid-data" } }
  }

  # authorships 超长处理
  ruby {
    code => "
      authors = event.get('authorships')
      if authors.is_a?(Array) && authors.length > 100
        event.set('authorships_full', authors)
        event.set('authorships', authors.slice(0,100))
        event.set('authorships_truncated', true)
      end
    "
  }
}

output {

  # Elasticsearch 写入
  elasticsearch {
    hosts => ["${ES_HOST_PROD}"]
    index => "works-v18-%{[@metadata][index_suffix]}"
    user => "${ES_USER_PROD}"
    password => "${ES_PASSWORD_PROD}"
    document_id => "%{id}"
  }
}
